Testing outliers
========================================================

```{r ustawienia ,echo=F}
opts_chunk$set(comment = "", warning = FALSE, message = FALSE, echo = TRUE, tidy = FALSE, size='tiny',dev='CairoPNG')
```

First, start with packages that we will use during our lecture.

* outliers
* extremeValues
* mvoutlier

Regression based methods
* car

Robust methods
* robustbase

First of course we need to start with visualising data. There are many methods to detect outliers in dataset. 

* simple boxplots
* density plots (kernel based)

```{r readpackages} 
library(ggplot2)
library(reshape2)
library(scales)
library(robustbase)
library(plyr)
library(car) ### influence plots
library(outliers)
library(extremevalues)
library(mvoutlier)
library(MASS)
```

Load the data

```{r loadData}
setwd('/home/berenz/Dokumenty/Projekty/Projekty_R/UNIVERSITY/Course Materials/REAL ESTATE/DataSets')
dane<-read.table('mieszkaniaWarszawa.csv',
                 dec=',',
                 header=T,
                 sep=';',              
                 na.strings='')

dane$dataDod<-paste('1',dane$miesiac,dane$rok,sep='-')

dane$dataDod<-as.Date(dane$dataDod,'%d-%m-%Y')
class(dane$dataDod)
```


Visual testing

```{r ggplots}
boxplot(dane$CenaM2,
        main='Distribution of Price per square meter',
        sub='Source: own calculations',
        ylab='Price per square meter')

### w czasie
ggplot(data=dane,aes(x=dataDod,
                     y=CenaM2,
          group=format(dataDod,'%Y-%m'))) +
  geom_boxplot()



p<-ggplot(data=dane,
       aes(x=dataDod,y=CenaM2,
           group=format(dataDod,'%Y-%m'))) +
  geom_boxplot(outlier.colour='red')+ 
  
  xlab('Add date') + ylab('Price per square meter') +
  ggtitle('Distribution of price per m2 over time')

p 

p + geom_smooth(method = "loess", 
                se=TRUE, 
                colour='red',
                aes(group=1))

p + geom_smooth(method = "lm", 
                se=TRUE, 
                colour='red',
                aes(group=1))


### zależność cena vs powierzchnia
p<-ggplot(data=dane,aes(x=pow,y=CenaM2)) + 
  geom_point() + xlab('Surface of flat') + ylab('Price per square meter') +
  ggtitle('Relation of surface and price per square meter')

p

p + geom_smooth(method='lm',aes(colour='red')) +
  geom_smooth(method='loess',aes(colour='blue')) +
  scale_colour_manual(name="Oszacowania", 
                breaks=c('red','blue'),
                labels=c('LM','LOESS'),
                values=c('red','blue'))

```

Data is really messy, outliers cause:
* nonlogical relation between surface and price per square meter (should be descending)
* influence on regression estimation 

Univariate outlier detections
-----------------

Let's start with simple outlier detection - one dimension outlier detection. For this we will use package outliers and extremevalues.

**extremevalues** packages contains function based on comparing to given distribution. There are two methods:

* Method I detects outliers by checking which are below (above) the limit where according to the model distribution less then rho[1] (rho[2]) observations are expected (given length(y) observations)
* Method II detects outliers by finding the observations (not used in the fit) who's fit residuals are below (above) the estimated confidence limit alpha[1] (alpha[2]) while all lower (higher) observations are outliers too.

To sum up - first method is based on the original values, second on residuals from distribution.

Now let see if any of variables has lognormal distribution

```{r lognormalvisualtest}
library(extremevalues)
par(mfrow=c(2,2))
plot(density(log(dane$pow)),main='log(Surface)')
plot(density(log(dane$CenaM2)),main='log(Price m2)')
plot(density(log(dane$CenaTransakcyjna)),main='log(Price)')
par(mfrow=c(1,1))
```

Density plots indicates that we deal with lognormal distributions.
Now, let use function getOutliers

```{r outliers}
method1<-getOutliers(dane$CenaM2,
                     method='I',
                     distribution="lognormal")

method2<-getOutliers(dane$CenaM2,
                     method='II',
                     distribution="lognormal")

par(mfrow=c(1,2))
outlierPlot(dane$CenaM2,method1,mode="qq")
outlierPlot(dane$CenaM2,method2,mode="residual")
par(mfrow=c(1,1))
```

In package *outliers* you can find statistical tests for outliers, but we will not discuss it.

* chisq.out.test
* cochran.test
* dixon.test
* grubbs.test

Multivariate distribution
--------------

First we use package **mvoutlier** and functions listed below:

* aq.plot - adjusted quantile plot
* chisq.plot -  chi-square plot
* corr.plot - robust bivariare correlation
* dd.plot - distance-distance plot


Use of adjusted quantile plot.

```{r mvquantileplot}
wyn<-aq.plot(dane[,c('CenaM2','pow')])
str(wyn)
```

Correlation plots.


res<-corr.plot(dane$pow,dane$CenaM2)






```{r mvcorrplot}
par(mfrow=c(1,2))
wyn<-corr.plot(dane$CenaM2,dane$pow)
wyn<-corr.plot(dane$CenaTransakcyjna,dane$pow)
par(mfrow=c(1,1))
```

The function dd.plot plots the classical mahalanobis distance of the data against the robust mahalanobis distance based on the mcd estimator.

```{r mvddplot}
wyn<-dd.plot(dane[,c('CenaM2','pow')])
```

The function symbol.plot plots the (two-dimensional) data using different symbols according to the robust mahalanobis distance based on the mcd estimator with adjustment.

```{r mvsymbolplot}
wyn<-symbol.plot(dane[,c('CenaM2','pow')])
```


Multivariate outliers based on regression measures
-------------

For this purpose we will use stats and car packages.

We will build a simple model price - surface



```{r model}
fit<-lm(log(CenaTransakcyjna)~pow,data=dane)
summary(fit)
```

Function returns:

**Leverage** given by
$$ h_i = \frac{1}{n} + \frac{(X_i - \overline{X})}{\sum(X_i - \overline{X})^2} $$

** Studentized residuals** given by
$$e_{i}^{*}=\frac{e_i}{S_{e(-1)}\sqrt{1-h_i}}$$

where:
$e_i$ - residual, $S_{e(-1)}$ - standard error of the regression without i-th observation. Studentized residuals follow t-distribution with $n-k-2$ degress of freedom.

**Cook distance**
$$ D_i=\frac{e_i}{k+1}\frac{h_i}{1-h_i}$$
where
k  -- number of dependent variables, $h_i = \frac{1}{n} + \frac{(X_i - \overline{X})}{\sum(X_i - \overline{X})^2}$ , $MSE=\frac{1}{n}\sum_{i=1}^n(\hat{Y}_i-Y_i)^2$
outliers meet:
$$ D_i>\frac{4}{n-k-1} $$

**DFBETA** measures change in estimates of regression parameters when we remove one observation

$$ DFBETA_i=(\sum_{i \in s} \mathbf{x}_i\mathbf{x}_i^T)^{-1}\mathbf{x}_i\frac{e_i}{1-\mathbf{x}_i^T(\sum_{i \in s} \mathbf{x}_i\mathbf{x}_i^T)^{-1}\mathbf{x}_i} $$

**DFBETAS** - standarised version of DFBETA. Measures influence in units of standard error   of regression.

$$ DFBETAS_i=\frac{\hat{\mathbf{\beta}}-\hat{\mathbf{\beta}}_{(-i)}}{\sqrt{MSE_{(-i)}}}=\frac{DFBETA_i}{\sqrt{MSE_{(-i)}}} $$

Outliers meet:

$$ |DFBETAS_i|>2 $$ - small samples
$$ DFBETAS_i>\frac{2}{\sqrt{n}} $$

**DFFITS** -- measures global difference between model with and without *i* observation.

$$ DFFITS_i=\frac{e_i\sqrt{\frac{h_i}{1-h_i}}}{\sqrt{MSE_{(-i)}}\sqrt{{1-h_i}}} \$$

Outliers meet $ |DFFITS_i| > 2\sqrt{\frac{p+1}{n-k-1}} $

**CovRatio** -- measures influence on variance of regression coefficients

$$ COVRATIO_i=\frac{1}{(\frac{n-k-2+t_i^2}{n-k-1})^{k+2}}\frac{1}{(1-h_i)} $$

where $$ h_i $$ is the same as in Cook's distance.
$$ t_i $$ is defined $$ t_i=\frac{e_i}{\sqrt{MSE_{(-i)}}\sqrt{{1-h_i}}} $$

Interpretation:

$$ COVRATIO_i < 1 $$ - elimination of *i* th unit/observation will reduce standard errors of regression coefficients
$$ COVRATIO_i > 1 $$ - elimination of *i* th unit/observation will increase standard errors of regression coefficients

it is suggested to use sample size dependent thresholds 

$$ |COVRATIO_i-1| > 3(k+1)/n  $$

```{r modeldiagnostics}
infMea<-influence.measures(fit)
infMea.df<-as.data.frame(infMea$infmat)
infMea.df$ID<-1:nrow(infMea.df)
infMeaLong<-melt(infMea.df,id.vars='ID')
ggplot(data=infMeaLong,aes(x=ID,y=value)) + 
  geom_point() +
  geom_line() +
  facet_wrap(~variable,ncol=1)

### findout potential outliers and plot them
dane$Outlier<-as.factor(as.numeric(rowSums(infMea$is.inf)>0))

p<-ggplot(data=dane,aes(x=pow,y=CenaTransakcyjna,color=Outlier)) +
  geom_point()

p + geom_smooth(method='lm',se=F,size=1,aes(color=Outlier))

ggplot(data=dane,aes(x=pow,y=CenaM2,color=Outlier)) +
  geom_point()


```

Compare results with and without outliers

```{r modelwithoutout}
fit1<-lm(log(CenaTransakcyjna)~pow,data=dane)
fit2<-lm(log(CenaTransakcyjna)~pow,data=subset(dane,Outlier==0))
summary(fit1)
summary(fit2)
par(mfrow=c(2,2))
plot(fit1)
plot(fit2)
par(mfrow=c(1,1))
```


If we do not want to detect outliers in our dataset we could use robust methods such as quantile regression to depend on median instead of mean or robust methods.

Let consider robust linear regression which could be used with MASS::rlm function.

Source: http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-robust-regression.pdf

Linear least-squares estimates can behave badly when the error distribution is not normal, particularly when the errors are heavy-tailed. One remedy is to remove influential observations from the least-squares fit.

Another approach, termed robust regression,is to employ a fitting criterion that is not as vulnerable as least squares to unusual data. The most common general method of robust regressionis M-estimation, introduced by Huber (1964). Consider the linear model:

$$y_i=\alpha+\beta_i x_{i1}+\beta_2 x_{i2}+ \ldots + \beta_k x_{ik} + \epsilon_i=\mathbf{x^'_i\beta}+\epsilon_i$$

for the ith of n observations. The fitted model is:

$$y_i=a + b_1x_{i1} + b_2 x_{i2} + \ldots + b_k x_{ik} + e_i=\mathbf{x^'_ib}+e_i$$

The general M-estimator minimizes the objective function 

$$\sum_{i=1}^{n}\rho(e_i)=\sum_{i=1}^{n}\rho(y_i-\mathbf{x^'_ib})$$

where the function $\rho$ gives the contribution of each residual to the objective function. Areasonable $\rho$ should have the following properties:

* $\rho(e) \geq 0$
* $\rho(0)=0$
* $\rho(e)=\rho(-e)$
* $\rho(e_i) \geq \rho(e_{i'})$ for $|e_i| > |e_{i'}|$

For example, for least-square regression, $\rho(e)=e_{i}^2$

Let $\psi=\rho'$ be the derivate of $\rho$. Differentiating the objective function with respect to the coefficients $\mathbf{b}$ and setting partial derivates to 0, produces a system of $k+1$ estimating equations for the coefficients:

$$\sum_{i=1}^{n}\psi(y_i-\mathbf{x^'_ib})\mathbf{x^'_i}=\mathbf{0}$$

Define the weight function $w(e)=\psi(e)/e$ and let $w_i=w(e_i)$. Then the estimating equations may be written as

$$\sum_{i=1}^{n}w_i(y_i-\mathbf{x^'_ib})\mathbf{x^'_i}=\mathbf{0})$$

Solving the estimating equations is a weighted least-squares problem, minimizing $\sum w_i^2e_i^2$. The  weights, however, depend upon the residuals, the residuals depend upon the estimated coefficients, and the estimated coefficients depend upon the weight.



```{r robustlm}
fit1<-lm(CenaM2~pow,data=dane)
fit2<-rlm(CenaM2~pow,data=dane)
summary(fit1)
summary(fit2)
```

Comparison of methods

```{r robustlm2}
fit2a<-rlm(CenaM2~pow,data=dane,method='M')
fit2b<-rlm(CenaM2~pow,data=dane,method='M',psi=psi.huber)
fit2c<-rlm(CenaM2~pow,data=dane,method='M',psi=psi.hampel)
fit2d<-rlm(CenaM2~pow,data=dane,method='M',psi=psi.bisquare)
coefficients(fit2a)
coefficients(fit2b)
coefficients(fit2c)
coefficients(fit2d)
```

Compare visually

```{r robustlm3}
plot(x=dane$pow,y=dane$CenaM2)
abline(coef=coefficients(fit2a),col='red')
abline(coef=coefficients(fit2b),col='blue')
abline(coef=coefficients(fit2c),col='green')
abline(coef=coefficients(fit2d),col='black')
```

Add weights to dane data.frame and use colour to detect outliers.

```{r robustlm4}
dane$Weights<-fit2a$w
ggplot(data=dane,aes(x=pow,y=CenaM2,colour=Weights)) + geom_point()
```

Now let compare two methods - least-square and robust least-square.

```{r rlmAndLm}
fit1<-lm(CenaTransakcyjna~pow,data=dane)
fit2<-rlm(CenaTransakcyjna~pow,data=dane)
lmcoeff<-as.numeric(coefficients(fit1))
rlmcoeff<-as.numeric(coefficients(fit2))

ggplot(data=dane,aes(x=pow,y=CenaTransakcyjna)) +
  geom_point() + 
  geom_abline(aes(intercept=lmcoeff[1],
                  slope=lmcoeff[2],
                  colour='red')) +
   geom_abline(aes(intercept=rlmcoeff[1],
                  slope=rlmcoeff[2],
                  colour='blue')) +
   scale_colour_manual(name="Oszacowania", 
                      breaks=c('red','blue'),
                      labels=c('LM','RLM'),
                      values=c('red','blue'))
```

The same for Price per square meter


```{r rlmAndLm2}
fit1<-lm(CenaM2~pow,data=dane)
fit2<-rlm(CenaM2~pow,data=dane)
summary(fit1)
summary(fit2)

lmcoeff<-as.numeric(coefficients(fit1))
rlmcoeff<-as.numeric(coefficients(fit2))

ggplot(data=dane,aes(x=pow,y=CenaM2)) +
  geom_point() + 
  geom_abline(aes(intercept=lmcoeff[1],
                  slope=lmcoeff[2],
                  colour='red')) +
   geom_abline(aes(intercept=rlmcoeff[1],
                  slope=rlmcoeff[2],
                  colour='blue')) +
   scale_colour_manual(name="Oszacowania", 
                      breaks=c('red','blue'),
                      labels=c('LM','RLM'),
                      values=c('red','blue'))
```
